# Modeling and Predicting Water Stress in Tunisia

Project goal: model and predict the level of water stress in Tunisia using World Bank environmental indicators. This repository demonstrates a full data-science workflow: data ingestion, cleaning, feature selection, time-aware model training, evaluation, and basic interpretation.

**Why this project matters**
- Water stress (freshwater withdrawals as a % of renewable freshwater resources) is a key sustainability indicator for Tunisia.
- Predicting water stress helps in planning, policy evaluation, and prioritizing interventions.

What this repo showcases
- Time-series aware machine learning with a clear temporal train/test split (train: 1960â€“2010, test: 2011â€“2024).
- Careful feature selection and preprocessing for World Bank long-format indicator data.
- Baseline models: Linear Regression, Decision Tree, Random Forest, with evaluation (MAE, RMSE, RÂ²).
- Diagnostic steps for multicollinearity and data leakage, with suggested mitigations.

Repository structure

Repository structure

```
tunisia_water_stress_ml/
â”œâ”€â”€ docs/                          # ðŸ“š Project documentation
â”‚   â”œâ”€â”€ PROJECT_HANDBOOK.md        # Comprehensive development guide
â”‚   â”œâ”€â”€ ARCHITECTURE.md            # System design + Mermaid diagrams
â”‚   â”œâ”€â”€ DECISIONS.md               # Design decisions & trade-offs
â”‚   â””â”€â”€ API.md                     # API endpoint reference
â”‚
â”œâ”€â”€ src/                           # ðŸ§  ML Pipeline (core logic)
â”‚   â”œâ”€â”€ data_loader.py             # Load World Bank CSVs â†’ wide format
â”‚   â”œâ”€â”€ preprocessing.py           # Clean, fill missing, select features
â”‚   â”œâ”€â”€ feature_engineering.py     # Create lags, temporal features
â”‚   â”œâ”€â”€ train.py                   # Orchestrate full pipeline
â”‚   â”œâ”€â”€ evaluate.py                # Metrics & visualization
â”‚   â”œâ”€â”€ predict_future.py          # Generate forecasts
â”‚   â”œâ”€â”€ hyperparameter_tuning.py   # GridSearchCV automation
â”‚   â””â”€â”€ feature_importance.py      # Feature analysis
â”‚
â”œâ”€â”€ api/                           # ðŸš€ FastAPI REST Service
â”‚   â”œâ”€â”€ main.py                    # App entry point
â”‚   â”œâ”€â”€ config.py                  # Configuration
â”‚   â”œâ”€â”€ schemas.py                 # Request/response models
â”‚   â”œâ”€â”€ model_service.py           # Model loading & inference
â”‚   â”œâ”€â”€ dependencies.py            # Dependency injection
â”‚   â”œâ”€â”€ logging_config.py          # Logging setup
â”‚   â””â”€â”€ routers/
â”‚       â”œâ”€â”€ v1.py                  # v1 endpoints
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/                         # âœ… Test Suite (52 tests)
â”‚   â”œâ”€â”€ test_data_loader.py        # Data loading tests (9)
â”‚   â”œâ”€â”€ test_preprocessing.py      # Preprocessing tests (16)
â”‚   â”œâ”€â”€ test_feature_engineering.py# Feature engineering tests (18)
â”‚   â”œâ”€â”€ test_pipeline_integration.py# End-to-end tests (8)
â”‚   â”œâ”€â”€ test_api.py                # API endpoint tests
â”‚   â”œâ”€â”€ test_model_service.py      # Model service tests
â”‚   â”œâ”€â”€ conftest.py                # Shared fixtures
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # World Bank indicator CSVs
â”‚   â”‚   â””â”€â”€ environment_tun.csv    # Tunisia environment data (long format)
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ processed_tunisia.csv  # Cleaned, wide-format (ready for ML)
â”‚
â”œâ”€â”€ models/                        # ðŸ“Š Trained Models & Results
â”‚   â”œâ”€â”€ RandomForest.joblib        # Trained model
â”‚   â”œâ”€â”€ Ridge.joblib               # Regularized linear model
â”‚   â”œâ”€â”€ metrics.csv                # Performance metrics (MAE, RMSE, RÂ²)
â”‚   â”œâ”€â”€ *_actual_vs_pred.png       # Prediction plots
â”‚   â””â”€â”€ *_feature_importance.png   # Feature importance plots
â”‚
â”œâ”€â”€ notebooks/                     # ðŸ““ Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb  # EDA, data quality checks
â”‚   â”œâ”€â”€ 02_model_inspection.ipynb  # Model plots & correlation analysis
â”‚   â””â”€â”€ 03_modeling.ipynb          # Full training walkthrough
â”‚
â”œâ”€â”€ .pre-commit-config.yaml        # Pre-commit hooks (black, flake8, mypy, bandit)
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â”œâ”€â”€ README.md                      # Project overview (you are here)
â”œâ”€â”€ CONTRIBUTING.md                # Contribution guidelines
â”œâ”€â”€ requirements.txt               # Production dependencies
â”œâ”€â”€ api_requirements.txt           # Dev + API dependencies
â”‚
â”œâ”€â”€ CODE_QUALITY_SUMMARY.md        # Code quality implementation details
â”œâ”€â”€ IMPLEMENTATION_STATUS.md       # Feature completion checklist
â”œâ”€â”€ FINAL_REPORT.md                # Project completion report
â”‚
â”œâ”€â”€ docker-compose.yml             # Multi-container orchestration
â”œâ”€â”€ Dockerfile                     # API container image
â”œâ”€â”€ nginx.conf                     # Reverse proxy configuration
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ check_correlations.py      # Correlation analysis utility
    â””â”€â”€ predict_2030.py            # Future predictions script
```

---

## Architecture Overview

**Data Flow:**
```
World Bank API (Open Data)
         â†“
   data/raw/*.csv
         â†“ (load_and_pivot)
   DataFrame (wide format)
         â†“ (preprocessing)
   Clean & selected features
         â†“ (feature_engineering)
   Lagged & temporal features
         â†“ (temporal split)
   Separate train (1960-2010) and test (2011-2024) sets
         â†“ (train.py)
   Trained models (.joblib) + metrics.csv
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Option 1: Batch (notebooks)         â”‚
   â”‚ Option 2: API (/v1/predict)         â”‚
   â”‚ Option 3: CLI (predict_future.py)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Temporal Split?** 
- âœ“ Prevents data leakage (no future info in training)
- âœ“ Realistic evaluation (how model performs on unseen future)
- âœ— (Incorrect) Random split would artificially inflate accuracy

## Quick Start

### Option 1: Run Training Pipeline

**Step 1: Set up environment**
```powershell
# Create virtual environment
python -m venv venv

# Activate (Windows)
& venv\Scripts\Activate.ps1

# Activate (macOS/Linux)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

**Step 2: Train models**
```powershell
# Basic training
python src/train.py

# With all options (recommended)
python src/train.py --models_dir models/ --tune_hyperparams --leakage_filter --collinearity_filter
```

**Step 3: Review results**
- Models saved to `models/` (`.joblib` files)
- Metrics in `models/metrics.csv`
- Plots in `models/` (`.png` files)
- Open `notebooks/02_model_inspection.ipynb` for visualization

### Option 2: Use REST API

**Start the API:**
```powershell
pip install -r api_requirements.txt
uvicorn api.main:app --reload
```

Navigate to `http://localhost:8000/docs` for interactive API documentation.

**Example predictions:**
```bash
curl -X POST "http://localhost:8000/v1/predict" \
  -H "Content-Type: application/json" \
  -d '{"features": {"indicator_1": 23.5, "year": 2023}, "model_name": "RandomForest"}'
```

### Option 3: Use Docker

```bash
# Build and run API container
docker-compose up -d

# API available at: http://localhost:8000
```

---

## Documentation

Complete guides available in `docs/`:

- **[PROJECT_HANDBOOK.md](docs/PROJECT_HANDBOOK.md)** â† Start here for detailed development guide
  - Development setup and environment configuration
  - Data pipeline walk-through with code examples
  - Model training options and results interpretation
  - API deployment and usage
  - Common tasks and troubleshooting

- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - System design and data flow
  - Data pipeline diagram (Mermaid)
  - System architecture diagram
  - Component interaction flow
  - Technology stack by layer
  - Deployment architecture
  - Scalability considerations

- **[DECISIONS.md](docs/DECISIONS.md)** - Why we chose specific technologies
  - Why Ridge/Lasso over XGBoost (with data size considerations)
  - Why temporal train/test split (prevents data leakage)
  - Multicollinearity and leakage detection strategies
  - Architecture choices (FastAPI, Docker, modular design)
  - Testing and quality assurance decisions

- **[CONTRIBUTING.md](CONTRIBUTING.md)** - How to contribute
  - Git workflow and commit message format
  - Branch naming conventions
  - Type hints and code quality requirements
  - Testing requirements (100% coverage on src/)
  - Pre-commit hooks setup

Key findings (from an initial run)
- The pipeline auto-detected a water-related target and trained three models. The saved metrics are in `models/metrics.csv` and plots are in `models/`.
- A near-perfect Linear Regression fit was traced to target leakage: the processed features contained indicators that are identical or direct transforms of the target (e.g., "Annual freshwater withdrawals, total (% of internal resources)" and similar). This causes inflated RÂ² and misleading performance.

Recommended next steps
- Remove duplicate/target-leaking indicators before training (drop columns that are identical or have |corr| >= 0.99 with the target).\
- Use regularized linear models (`Ridge`, `Lasso`) to reduce coefficient instability.\
- Use feature selection (drop highly collinear features / use PCA) and hyperparameter tuning for tree ensembles.\
- Expand evaluation: cross-validate using rolling-origin (time-series CV), and produce explainability plots (SHAP) for Random Forest.

What I learned / Demonstrated skills
- Practical handling of long-form World Bank data and reshaping to wide time-series.\
- Time-aware splitting and the importance of avoiding random splits on time-series data.\
- Detecting data leakage and diagnosing multicollinearity using correlation matrices.\
- Building a small, reproducible training pipeline with clear outputs (models, plots, metrics).

Notes and caveats
- The current pipeline is Stage 1 (baselines and diagnostics). Careful feature curation and model tuning are required before trustable policy recommendations can be made.\
- Some World Bank indicators may be sparse or only partially overlapping in years â€” the preprocessing step interpolates and forward/back-fills where appropriate; review interpolation choices for your analysis goals.

If you want, I can now:
- automatically drop columns that leak the target and re-run training with `Ridge`, or\
- produce a minimal write-up section suitable for GitHub (results, plots, and interpretation).

Contact
- Repo maintained by the project author. Pull requests and issues welcome.

